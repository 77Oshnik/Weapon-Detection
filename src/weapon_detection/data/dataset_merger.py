"""Dataset merger for combining multiple YOLO datasets."""

import shutil
import yaml
from pathlib import Path
from typing import Dict, List
from collections import defaultdict

from ..utils.config import load_config\n\n\nclass DatasetMerger:\n    \"\"\"Merge multiple YOLO datasets into a unified dataset.\"\"\"\n    \n    def __init__(self, config_path: Path):\n        \"\"\"Initialize dataset merger.\n        \n        Args:\n            config_path: Path to merge configuration file\n        \"\"\"\n        self.config = load_config(config_path)\n        self.unified_classes = self.config['unified_classes']\n        self.datasets = self.config['datasets']\n    \n    def merge(self, output_dir: Path, dry_run: bool = False) -> Dict[str, Dict[str, int]]:\n        \"\"\"Merge all datasets into unified structure.\n        \n        Args:\n            output_dir: Output directory for merged dataset\n            dry_run: If True, show what would be done without copying\n            \n        Returns:\n            Statistics dictionary\n        \"\"\"\n        print(f\"[INFO] Merging {len(self.datasets)} datasets into {output_dir}\")\n        print(f\"[INFO] Unified classes: {self.unified_classes}\")\n        \n        if dry_run:\n            print(\"[DRY RUN] Would merge the following datasets:\")\n            for dataset in self.datasets:\n                print(f\"  - {dataset['name']}: {dataset['path']}\")\n            return {}\n        \n        # Create output structure\n        for split in ['train', 'val', 'test']:\n            (output_dir / split / 'images').mkdir(parents=True, exist_ok=True)\n            (output_dir / split / 'labels').mkdir(parents=True, exist_ok=True)\n        \n        # Process each dataset\n        total_stats = defaultdict(lambda: defaultdict(int))\n        \n        for dataset_config in self.datasets:\n            print(f\"\\n[INFO] Processing dataset: {dataset_config['name']}\")\n            \n            for split in ['train', 'val', 'test']:\n                images, labels = self._process_dataset_split(\n                    dataset_config, output_dir, split\n                )\n                total_stats[split]['images'] += images\n                total_stats[split]['labels'] += labels\n        \n        # Create unified data.yaml\n        self._create_data_yaml(output_dir)\n        \n        # Print summary\n        print(f\"\\n[SUMMARY] Unified dataset created in {output_dir}\")\n        for split in ['train', 'val', 'test']:\n            stats = total_stats[split]\n            print(f\"  {split}: {stats['images']} images, {stats['labels']} labels\")\n        \n        total_images = sum(stats['images'] for stats in total_stats.values())\n        total_labels = sum(stats['labels'] for stats in total_stats.values())\n        print(f\"  Total: {total_images} images, {total_labels} labels\")\n        \n        return dict(total_stats)\n    \n    def _process_dataset_split(self, dataset_config: Dict, output_dir: Path, \n                              split_name: str) -> tuple:\n        \"\"\"Process one dataset split.\"\"\"\n        dataset_name = dataset_config['name']\n        dataset_path = Path(dataset_config['path'])\n        class_mapping = dataset_config['class_mapping']\n        \n        # Handle val/valid naming\n        src_split = 'valid' if split_name == 'val' else split_name\n        src_images_dir = dataset_path / src_split / 'images'\n        src_labels_dir = dataset_path / src_split / 'labels'\n        \n        # Fallback for different naming conventions\n        if not src_images_dir.exists() and split_name == 'val':\n            src_images_dir = dataset_path / 'val' / 'images'\n            src_labels_dir = dataset_path / 'val' / 'labels'\n        \n        if not src_images_dir.exists():\n            print(f\"[WARN] {dataset_name} {split_name} not found: {src_images_dir}\")\n            return 0, 0\n        \n        # Destination paths\n        dst_images_dir = output_dir / split_name / 'images'\n        dst_labels_dir = output_dir / split_name / 'labels'\n        \n        copied_images = 0\n        copied_labels = 0\n        \n        # Process all images\n        image_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.tif'}\n        for img_path in src_images_dir.iterdir():\n            if img_path.suffix.lower() not in image_extensions:\n                continue\n            \n            # Copy image with dataset prefix\n            dst_img_name = f\"{dataset_name}_{img_path.name}\"\n            dst_img_path = dst_images_dir / dst_img_name\n            shutil.copy2(img_path, dst_img_path)\n            copied_images += 1\n            \n            # Process label file\n            src_label_path = src_labels_dir / f\"{img_path.stem}.txt\"\n            dst_label_path = dst_labels_dir / f\"{dst_img_name.rsplit('.', 1)[0]}.txt\"\n            \n            if src_label_path.exists():\n                # Remap class IDs\n                new_lines = self._remap_label_file(src_label_path, class_mapping)\n                with open(dst_label_path, 'w', encoding='utf-8') as f:\n                    f.write('\\n'.join(new_lines))\n                    if new_lines:\n                        f.write('\\n')\n            else:\n                # Create empty label file\n                dst_label_path.write_text('', encoding='utf-8')\n            \n            copied_labels += 1\n        \n        print(f\"[INFO] {dataset_name} {split_name}: {copied_images} images, {copied_labels} labels\")\n        return copied_images, copied_labels\n    \n    def _remap_label_file(self, label_path: Path, class_mapping: Dict[int, int]) -> List[str]:\n        \"\"\"Remap class IDs in a YOLO label file.\"\"\"\n        if not label_path.exists():\n            return []\n        \n        new_lines = []\n        with open(label_path, 'r', encoding='utf-8') as f:\n            for line in f:\n                line = line.strip()\n                if not line:\n                    continue\n                \n                parts = line.split()\n                if len(parts) >= 5:\n                    old_class_id = int(parts[0])\n                    if old_class_id in class_mapping:\n                        new_class_id = class_mapping[old_class_id]\n                        parts[0] = str(new_class_id)\n                        new_lines.append(' '.join(parts))\n        \n        return new_lines\n    \n    def _create_data_yaml(self, output_dir: Path):\n        \"\"\"Create unified data.yaml file.\"\"\"\n        data_yaml_content = f\"\"\"train: train/images\nval: val/images\ntest: test/images\n\nnc: {len(self.unified_classes)}\nnames: {self.unified_classes}\n\n# Unified weapon detection dataset\n# Classes: {', '.join(self.unified_classes)}\n\"\"\"\n        \n        data_yaml_path = output_dir / 'data.yaml'\n        data_yaml_path.write_text(data_yaml_content, encoding='utf-8')\n        print(f\"[INFO] Created unified data.yaml with {len(self.unified_classes)} classes\")